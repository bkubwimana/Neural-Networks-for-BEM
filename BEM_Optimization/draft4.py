# -*- coding: utf-8 -*-
"""draft4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_gb469vA6xGv976PzImL7XZFtj375Klv
"""

# mlp for multi-output regression
from numpy import mean
from numpy import std
from sklearn.datasets import make_regression
from sklearn.model_selection import RepeatedKFold
from keras.models import Sequential
from keras.layers import Dense,Dropout, BatchNormalization
import pandas as pd
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.optimizers import SGD
from matplotlib import pyplot
from tensorflow.keras.metrics import RootMeanSquaredError
from keras import backend as K
from sklearn.model_selection import KFold
import numpy as np
from sklearn.multioutput import MultiOutputRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, mean_absolute_error
import math
from tensorflow import keras
import pickle 
import copy

from google.colab import drive
drive.mount('/content/gdrive')

cd /content/gdrive/MyDrive/Colab Notebooks/Genetic Opt

df= pd.read_csv('./input_output.csv')

X= df[df.columns[:8]].values
y= df[['Electricity:Facility','Photovoltaic:ElectricityProduced','ElectricityPurchased:Facility']].values

y.mean(axis=0)
y.min(axis=0)
y.max(axis=0)

# created scaler
scaler_x = StandardScaler()
# fit scaler on training dataset
scaler_x.fit(X)
# transform dataset
X_scaled = scaler_x.transform(X)

# created scaler
scaler_y = StandardScaler()
# fit scaler on training dataset
scaler_y.fit(y)
# transform dataset
y_scaled = scaler_y.transform(y)

# split into train and test
n_train = 80
trainX, testX = X_scaled[:n_train, :], X_scaled[n_train:, :]
trainy, testy = y_scaled[:n_train], y_scaled[n_train:]

def root_mean_squared_error(y_true, y_pred):
        return K.sqrt(K.mean(K.square(y_pred - y_true)))

"""# Model training"""

# get the model
def get_model(n_inputs, n_outputs):
    model = Sequential()
    model.add(Dense(128, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(64,activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(32,activation='relu'))
    model.add(Dense(16,activation='relu'))
    model.add(Dense(8,activation='relu'))
    model.add(Dense(4,activation='relu'))
    #model.add(Dense(4,activation='relu'))
    model.add(Dense(n_outputs))
    model.compile(loss=root_mean_squared_error, optimizer='adam')
#     model.compile(loss='mae', optimizer=SGD(lr=0.01, momentum=0.9))
    return model

n_inputs, n_outputs = X.shape[1], y.shape[1]
# get model

model = get_model(n_inputs, n_outputs)
# fit the model on all data
history= model.fit(trainX, trainy, verbose=1, epochs=1000)
# evaluate the model
train_mse = model.evaluate(trainX, trainy, verbose=0)
test_mse = model.evaluate(testX, testy, verbose=0)

print('Train: %.3f, Test: %.3f' % ( train_mse, test_mse))
# plot loss during training
pyplot.title('Root Mean Squared Error')
pyplot.plot(history.history['loss'], label='train')
#pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
pyplot.show()

"""# save model"""

#saving weights and model
model.save("./my_model")

#save scaler
with open("my_scaler_x.pkl","wb") as f:
    pickle.dump(scaler_x, f)

with open("my_scaler_y.pkl","wb") as f:
    pickle.dump(scaler_y, f)

"""# test"""

# It can be used to reconstruct the model identically.
reconstructed_model = keras.models.load_model("./my_model",compile=False)

#load scaler
with open("my_scaler_x.pkl","rb") as f:
    loaded_scaler_x= pickle.load(f)

with open("my_scaler_y.pkl","rb") as f:
    loaded_scaler_y= pickle.load(f)

#example = X[-1]
example= copy.deepcopy(X[-1])
example[[0]] = 10
example= example[np.newaxis,:]
example_scaled= loaded_scaler_x.transform(example)
#let's check
pred= loaded_scaler_y.inverse_transform(reconstructed_model.predict(example_scaled))
pred

